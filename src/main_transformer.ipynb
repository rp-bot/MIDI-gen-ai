{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import chord, note, stream, clef, meter\n",
    "from data_cleaning import Open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid_file1 = os.path.join(os.getcwd(), \"progression1.mid\")\n",
    "# midi_data1 = Open.open_midi(mid_file1)\n",
    "# mid_file2 = os.path.join(os.getcwd(), \"progression2.mid\")\n",
    "# midi_data2 = Open.open_midi(mid_file2)\n",
    "\n",
    "concatenated_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_midi_files(dest_directory):\n",
    "    for root, dirs, files in os.walk(dest_directory):\n",
    "        for file in files:\n",
    "            mid_file = os.path.join(root, file)\n",
    "            midi_data = Open.open_midi(mid_file)\n",
    "            for i, part in enumerate(midi_data.parts):\n",
    "                for element in part.recurse():\n",
    "                    if isinstance(element, chord.Chord):\n",
    "                        concatenated_array.append(element)\n",
    "\n",
    "\n",
    "open_midi_files(os.path.join(os.getcwd(), \"sample_rock_set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6538"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concatenated_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chords = []\n",
    "for _chord in concatenated_array:\n",
    "    chord_arr = []\n",
    "    for _note in _chord:\n",
    "        chord_arr.append(_note.pitch.ps)\n",
    "    all_chords.append(chord_arr)\n",
    "\n",
    "flattened_list = [\n",
    "    int(each_note) for each_chord in all_chords for each_note in each_chord\n",
    "]\n",
    "notes = sorted(set(flattened_list))\n",
    "n_to_i = {s: i + 1 for i, s in enumerate(notes)}\n",
    "n_to_i[\".\"] = 0\n",
    "\n",
    "i_to_n = {value: key for key, value in n_to_i.items()}\n",
    "\n",
    "mn_to_nn = {n: note.Note(n) for n in range(128)}\n",
    "\n",
    "vocab_size = len(i_to_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(twod_chord_list):\n",
    "    return [n_to_i[n] for _chord_list in twod_chord_list for n in _chord_list + [\".\"]]\n",
    "\n",
    "\n",
    "def decoder(list_of_keys):\n",
    "    return [i_to_n[i] for i in list_of_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 26, 19, 0, 31, 26, 19, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(all_chords[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25994]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "encoded_chords = torch.tensor(encoder(all_chords), dtype=torch.long)\n",
    "print(encoded_chords.shape, encoded_chords.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(encoded_chords))\n",
    "train_data = encoded_chords[:n]\n",
    "val_data = encoded_chords[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3489764398)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, Yb = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[32, 25, 22, 17, 10,  0, 34, 32],\n",
       "         [10,  0, 17, 10,  0, 17, 10,  0],\n",
       "         [29, 25,  0, 20, 13,  0, 34, 30],\n",
       "         [20, 15,  8,  0, 25, 20, 15,  8]]),\n",
       " tensor([[25, 22, 17, 10,  0, 34, 32, 25],\n",
       "         [ 0, 17, 10,  0, 17, 10,  0, 17],\n",
       "         [25,  0, 20, 13,  0, 34, 30, 25],\n",
       "         [15,  8,  0, 25, 20, 15,  8,  0]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb, Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(54865)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, C = logits.shape\n",
    "        #     logits = logits.view(B*T, C)\n",
    "        #     targets = targets.view(B*T)\n",
    "        #     loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = m(Xb, Yb)\n",
    "\n",
    "logits, loss\n",
    "# logits, loss = m(Xb, Yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "decoder(m.generate(idx=torch.zeros(\n",
    "    (1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7502195835113525\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =decoder(m.generate(idx=torch.zeros(\n",
    "    (1, 1), dtype=torch.long), max_new_tokens=16)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_to_nn[\".\"] = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mn in x:\n",
    "    print(mn_to_nn[mn])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(654651)\n",
    "B,T,C = 4,8,2\n",
    "X = torch.randn(B,T,C)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xboc = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = X[b,:t+1]\n",
    "        xboc[b,t] = torch.mean(xprev,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8342,  1.3786],\n",
       "         [-0.0693, -0.2892],\n",
       "         [ 1.0316,  1.5584],\n",
       "         [ 0.9983,  0.1137],\n",
       "         [-1.6205,  0.7124],\n",
       "         [-1.4671, -0.6002],\n",
       "         [-0.1404, -0.9244],\n",
       "         [-0.5072, -0.1338]]),\n",
       " tensor([[-0.8342,  1.3786],\n",
       "         [-0.4518,  0.5447],\n",
       "         [ 0.0427,  0.8826],\n",
       "         [ 0.2816,  0.6904],\n",
       "         [-0.0988,  0.6948],\n",
       "         [-0.3269,  0.4790],\n",
       "         [-0.3002,  0.2785],\n",
       "         [-0.3261,  0.2269]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], xboc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIDI-gen-ai-JpLec6SW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
