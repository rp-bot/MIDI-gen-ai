{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import chord, note, stream, clef, meter\n",
    "from data_cleaning import Open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_file1 = os.path.join(os.getcwd(), \"progression1.mid\")\n",
    "midi_data1 = Open.open_midi(mid_file1)\n",
    "mid_file2 = os.path.join(os.getcwd(), \"progression2.mid\")\n",
    "midi_data2 = Open.open_midi(mid_file2)\n",
    "\n",
    "concatenated_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<music21.chord.Chord C3 G3 C4 D4 E4>, <music21.chord.Chord G2 G3 B3 D4 E4>, <music21.chord.Chord A2 G3 A3 C4 E4>, <music21.chord.Chord F2 C4 F4 G4>, <music21.chord.Chord C3 G3 C4 D4>, <music21.chord.Chord A2 A3 C4 E4>, <music21.chord.Chord F2 A3 C4 F4>, <music21.chord.Chord G2 G3 B3 E4>, <music21.chord.Chord C3 G3 B3 E4 G4>, <music21.chord.Chord F2 A3 C4 E4 G4>]\n"
     ]
    }
   ],
   "source": [
    "def open_midi_files(dest_directory):\n",
    "    for root, dirs, files in os.walk(dest_directory):\n",
    "        for file in files:\n",
    "            mid_file = os.path.join(root, file)\n",
    "            midi_data = Open.open_midi(mid_file)\n",
    "            for i, part in enumerate(midi_data.parts):\n",
    "                for element in part.recurse():\n",
    "                    if isinstance(element, chord.Chord):\n",
    "                        concatenated_array.append(element)\n",
    "\n",
    "\n",
    "open_midi_files(os.path.join(os.getcwd(), \"sample_rock_set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6724"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concatenated_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chords = []\n",
    "for _chord in concatenated_array:\n",
    "    chord_arr = []\n",
    "    for _note in _chord:\n",
    "        chord_arr.append(_note.pitch.ps)\n",
    "    all_chords.append(chord_arr)\n",
    "\n",
    "flattened_list = [\n",
    "    int(each_note) for each_chord in all_chords for each_note in each_chord\n",
    "]\n",
    "notes = sorted(set(flattened_list))\n",
    "n_to_i = {s: i + 1 for i, s in enumerate(notes)}\n",
    "n_to_i[\".\"] = 0\n",
    "\n",
    "i_to_n = {value: key for key, value in n_to_i.items()}\n",
    "\n",
    "mn_to_nn = {n: note.Note(n) for n in range(128)}\n",
    "\n",
    "vocab_size = len(i_to_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(twod_chord_list):\n",
    "    return [n_to_i[n] for _chord_list in twod_chord_list for n in _chord_list + [\".\"]]\n",
    "\n",
    "\n",
    "def decoder(list_of_keys):\n",
    "    return [i_to_n[i] for i in list_of_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 25, 30, 32, 34, 0, 13, 25, 29, 32, 34, 0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(all_chords[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26985]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "encoded_chords = torch.tensor(encoder(all_chords), dtype=torch.long)\n",
    "print(encoded_chords.shape, encoded_chords.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(encoded_chords))\n",
    "train_data = encoded_chords[:n]\n",
    "val_data = encoded_chords[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3489764398)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, Yb = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[29, 24, 17,  0, 22, 17, 10,  0],\n",
       "         [22, 15,  0, 27, 22, 15,  0, 27],\n",
       "         [24, 17,  0, 26, 19,  0, 24, 17],\n",
       "         [ 0, 11, 18, 23,  0, 11, 18, 23]]),\n",
       " tensor([[24, 17,  0, 22, 17, 10,  0, 22],\n",
       "         [15,  0, 27, 22, 15,  0, 27, 22],\n",
       "         [17,  0, 26, 19,  0, 24, 17,  0],\n",
       "         [11, 18, 23,  0, 11, 18, 23,  0]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb, Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2089, -0.7427,  0.1015,  ...,  0.6750,  1.5664, -0.9238],\n",
       "         [-1.7823,  0.1339, -2.0973,  ..., -0.2614,  0.9901,  0.6409],\n",
       "         [-0.0755,  0.4162,  0.5739,  ..., -1.0974, -0.0379,  1.5241],\n",
       "         ...,\n",
       "         [-0.4325, -0.2694, -2.5596,  ...,  0.2330, -0.2205, -1.4869],\n",
       "         [ 0.2110,  1.3096, -0.6448,  ..., -0.1489, -0.7810,  0.5884],\n",
       "         [ 1.6347, -0.0518,  0.4996,  ...,  0.3608,  0.3161,  0.3504]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(4.6719, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, C = logits.shape\n",
    "        #     logits = logits.view(B*T, C)\n",
    "        #     targets = targets.view(B*T)\n",
    "        #     loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # return logits, loss\n",
    "\n",
    "    # def generate(self, idx, max_new_tokens):\n",
    "    #     # idx is (B, T) array of indices in the current context\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         # get the predictions\n",
    "    #         logits, loss = self(idx)\n",
    "    #         # focus only on the last time step\n",
    "    #         logits = logits[:, -1, :] # becomes (B, C)\n",
    "    #         # apply softmax to get probabilities\n",
    "    #         probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "    #         # sample from the distribution\n",
    "    #         idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "    #         # append sampled index to the running sequence\n",
    "    #         idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    #     return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = m(Xb, Yb)\n",
    "\n",
    "logits, loss\n",
    "# logits, loss = m(Xb, Yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "# print(decoder(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIDI-gen-ai-JpLec6SW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
